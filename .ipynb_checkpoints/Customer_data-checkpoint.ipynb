{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2bb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "import boto3\n",
    "import schedule\n",
    "import time\n",
    "import import_ipynb\n",
    "import nbimporter\n",
    "from faker import Faker\n",
    "from statistics import mean\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c487dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2023-05-01 saved to conversation_data_2023-05-01.csv\n",
      "Data for 2023-05-02 saved to conversation_data_2023-05-02.csv\n",
      "Data for 2023-05-03 saved to conversation_data_2023-05-03.csv\n",
      "Data for 2023-05-04 saved to conversation_data_2023-05-04.csv\n",
      "Data for 2023-05-05 saved to conversation_data_2023-05-05.csv\n",
      "Data for 2023-05-06 saved to conversation_data_2023-05-06.csv\n",
      "Data for 2023-05-07 saved to conversation_data_2023-05-07.csv\n",
      "Data for 2023-05-08 saved to conversation_data_2023-05-08.csv\n",
      "Data for 2023-05-09 saved to conversation_data_2023-05-09.csv\n",
      "Data for 2023-05-10 saved to conversation_data_2023-05-10.csv\n",
      "Data for 2023-05-11 saved to conversation_data_2023-05-11.csv\n",
      "Data for 2023-05-12 saved to conversation_data_2023-05-12.csv\n",
      "Data for 2023-05-13 saved to conversation_data_2023-05-13.csv\n",
      "Data for 2023-05-14 saved to conversation_data_2023-05-14.csv\n",
      "Data for 2023-05-15 saved to conversation_data_2023-05-15.csv\n",
      "Data for 2023-05-16 saved to conversation_data_2023-05-16.csv\n",
      "Data for 2023-05-17 saved to conversation_data_2023-05-17.csv\n",
      "Data for 2023-05-18 saved to conversation_data_2023-05-18.csv\n",
      "Data for 2023-05-19 saved to conversation_data_2023-05-19.csv\n",
      "Data for 2023-05-20 saved to conversation_data_2023-05-20.csv\n",
      "Data for 2023-05-21 saved to conversation_data_2023-05-21.csv\n",
      "Data for 2023-05-22 saved to conversation_data_2023-05-22.csv\n",
      "Data for 2023-05-23 saved to conversation_data_2023-05-23.csv\n",
      "Data for 2023-05-24 saved to conversation_data_2023-05-24.csv\n",
      "Data for 2023-05-25 saved to conversation_data_2023-05-25.csv\n",
      "Data for 2023-05-26 saved to conversation_data_2023-05-26.csv\n",
      "Data for 2023-05-27 saved to conversation_data_2023-05-27.csv\n",
      "Data for 2023-05-28 saved to conversation_data_2023-05-28.csv\n",
      "Data for 2023-05-29 saved to conversation_data_2023-05-29.csv\n",
      "Data for 2023-05-30 saved to conversation_data_2023-05-30.csv\n",
      "Data for 2023-05-31 saved to conversation_data_2023-05-31.csv\n",
      "Data for 2023-06-01 saved to conversation_data_2023-06-01.csv\n",
      "Data for 2023-06-02 saved to conversation_data_2023-06-02.csv\n",
      "Data for 2023-06-03 saved to conversation_data_2023-06-03.csv\n",
      "Data for 2023-06-04 saved to conversation_data_2023-06-04.csv\n",
      "Data for 2023-06-05 saved to conversation_data_2023-06-05.csv\n",
      "Data for 2023-06-06 saved to conversation_data_2023-06-06.csv\n",
      "Data for 2023-06-07 saved to conversation_data_2023-06-07.csv\n",
      "Data for 2023-06-08 saved to conversation_data_2023-06-08.csv\n",
      "Data for 2023-06-09 saved to conversation_data_2023-06-09.csv\n",
      "Data for 2023-06-10 saved to conversation_data_2023-06-10.csv\n",
      "Data for 2023-06-11 saved to conversation_data_2023-06-11.csv\n",
      "Data for 2023-06-12 saved to conversation_data_2023-06-12.csv\n",
      "Data for 2023-06-13 saved to conversation_data_2023-06-13.csv\n",
      "Data for 2023-06-14 saved to conversation_data_2023-06-14.csv\n",
      "Data for 2023-06-15 saved to conversation_data_2023-06-15.csv\n",
      "Data for 2023-06-16 saved to conversation_data_2023-06-16.csv\n",
      "Data for 2023-06-17 saved to conversation_data_2023-06-17.csv\n",
      "Data for 2023-06-18 saved to conversation_data_2023-06-18.csv\n",
      "Data for 2023-06-19 saved to conversation_data_2023-06-19.csv\n",
      "Data for 2023-06-20 saved to conversation_data_2023-06-20.csv\n",
      "Data for 2023-06-21 saved to conversation_data_2023-06-21.csv\n",
      "Data for 2023-06-22 saved to conversation_data_2023-06-22.csv\n",
      "Data for 2023-06-23 saved to conversation_data_2023-06-23.csv\n",
      "Data for 2023-06-24 saved to conversation_data_2023-06-24.csv\n",
      "Data for 2023-06-25 saved to conversation_data_2023-06-25.csv\n",
      "Data for 2023-06-26 saved to conversation_data_2023-06-26.csv\n",
      "Data for 2023-06-27 saved to conversation_data_2023-06-27.csv\n",
      "Data for 2023-06-28 saved to conversation_data_2023-06-28.csv\n",
      "Data for 2023-06-29 saved to conversation_data_2023-06-29.csv\n",
      "Data for 2023-06-30 saved to conversation_data_2023-06-30.csv\n"
     ]
    }
   ],
   "source": [
    "faker = Faker()\n",
    "\n",
    "def generate_conversation_data(start_date, end_date):\n",
    "    data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        data.append({\n",
    "            'customer_id': faker.uuid4(),\n",
    "            'conversation_id': faker.uuid4(),\n",
    "            'agent_id': faker.uuid4(),\n",
    "            'call_start': current_date,\n",
    "            'call_end': current_date + timedelta(minutes=random.randint(1, 30)),\n",
    "            'call_duration_sec': random.randint(30, 600),\n",
    "            'call_status': random.choice(['Answered', 'Missed', 'Rejected', 'Hangup', 'Voicemail']),\n",
    "            'transcript': faker.sentence(),\n",
    "            'sentiment_score': random.uniform(0, 1),\n",
    "            'keywords': [faker.word() for _ in range(random.randint(1, 5))],\n",
    "            'created_at': datetime.now(),\n",
    "            'updated_at': datetime.now()\n",
    "        })\n",
    "        current_date += timedelta(days=1)\n",
    "    return data\n",
    "\n",
    "# Set the start and end dates for generating data\n",
    "start_date = datetime(2023, 5, 1)\n",
    "end_date = datetime(2023, 6, 30)\n",
    "\n",
    "# Generate conversation data\n",
    "synthetic_data = generate_conversation_data(start_date, end_date)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Group the data by date and save one file per day\n",
    "for date, group in df.groupby(df['call_start'].dt.date):\n",
    "    file_name = f\"conversation_data_{date}.csv\"\n",
    "    group.to_csv(file_name, index=False)\n",
    "    print(f\"Data for {date} saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643a110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the conversation data from CSV files (assuming one file per day)\n",
    "file_names = ['conversation_data_2023-05-01.csv', 'conversation_data_2023-06-02.csv']  # Add more file names as needed\n",
    "\n",
    "data = pd.concat([pd.read_csv(file_name) for file_name in file_names], ignore_index=True)\n",
    "\n",
    "# Calculate metrics for each agent\n",
    "metrics = data.groupby('agent_id').agg({\n",
    "    'call_duration_sec': ['mean', lambda x: np.percentile(x, 90)]\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for the metrics\n",
    "metrics.columns = ['agent_id', 'average_call_length', '90th_percentile_call_length']\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics.to_csv('metrics1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28a9f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from s3_upload.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import the `upload_csv_to_s3` function from the notebook\n",
    "from s3_upload import upload_csv_to_s3\n",
    "\n",
    "# Specify the necessary parameters\n",
    "csv_filename = 'metrics1.csv'\n",
    "bucket_name = 'conversationcustomerdata'\n",
    "\n",
    "# Call the upload_csv_to_s3 function\n",
    "upload_csv_to_s3(csv_filename, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc98dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not yet time to execute the daily task.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def run_daily_task():\n",
    "    synthetic_data = generate_conversation_data(start_date, end_date)\n",
    "    print(\"Synthetic data generated.\")\n",
    "    \n",
    "    metrics = calculate_metrics(synthetic_data)\n",
    "    print(\"Metrics calculated.\")\n",
    "    \n",
    "    csv_filename = 'metrics.csv'\n",
    "    write_metrics_to_csv(metrics, csv_filename)\n",
    "    print(\"Metrics written to CSV file.\")\n",
    "    \n",
    "    bucket_name = 'conversationcustomerdata'\n",
    "    upload_csv_to_s3(csv_filename, bucket_name)\n",
    "    print(\"CSV file uploaded to S3.\")\n",
    "\n",
    "# Check if it's time to execute the task\n",
    "current_time = datetime.datetime.now().strftime(\"%H:%M\")\n",
    "scheduled_time = \"16:15\"  # Set the desired scheduled time\n",
    "\n",
    "if current_time >= scheduled_time:\n",
    "    print(\"Executing daily task...\")\n",
    "    run_daily_task()\n",
    "else:\n",
    "    print(\"It's not yet time to execute the daily task.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdac57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4138c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
